{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7638eebf",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "\n",
    "En este cuaderno, vamos a realizar un preprocesamiento de datos utilizando `scikit-learn`. El preprocesamiento es una fase crítica en cualquier pipeline de machine learning, ya que asegura que los datos estén en un formato adecuado para entrenar modelos. Al final, también veremos cómo almacenar los objetos de preprocesamiento utilizando `joblib` y por qué es importante dentro de las prácticas de MLOps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6eb5c",
   "metadata": {},
   "source": [
    "## Importación de Librerías\n",
    "Primero, importamos todas las librerías necesarias. Estas incluyen herramientas de manipulación de datos (`pandas`, `numpy`), visualización (`seaborn`, `matplotlib`), y preprocesamiento (`scikit-learn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d868426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cdfcc6",
   "metadata": {},
   "source": [
    "## Carga de los Datos\n",
    "Vamos a cargar un conjunto de datos que contiene información demográfica. Este dataset proviene de `data_adults.csv`, un conjunto de datos comúnmente utilizado para predecir el nivel de ingresos basado en características demográficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "Data = pd.read_csv(\"../Datos/data_adults.csv\")\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbdb26",
   "metadata": {},
   "source": [
    "## Exploración de Datos\n",
    "Es importante conocer la estructura de los datos antes de preprocesarlos. Vamos a visualizar las primeras filas y también la estructura de tipos de datos en cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda69245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vista rápida de los datos\n",
    "print(Data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806d2a0",
   "metadata": {},
   "source": [
    "## Eliminación de Columnas Irrelevantes\n",
    "En muchos casos, algunos datos no aportan valor a nuestro modelo o pueden estar altamente correlacionados con otras variables. Aquí, eliminamos la columna `fnlwgt` (un peso final de la muestra) y `education-num` (número de años de educación, que ya está representado en la columna `education`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82841321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columnas irrelevantes\n",
    "Data_cop = Data.drop(\"fnlwgt\", axis=1)\n",
    "Data_cop = Data_cop.drop(\"education-num\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3737370d",
   "metadata": {},
   "source": [
    "## Separación de Variables y Objetivo\n",
    "Dividimos los datos en dos partes: las características (`X`) y la variable objetivo (`y`), que en este caso es la columna `income`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Data_cop.drop(\"income\", axis=1)\n",
    "y = Data_cop['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f93c40b",
   "metadata": {},
   "source": [
    "## Definición de Estrategia de Preprocesamiento\n",
    "Aquí definimos diferentes estrategias de preprocesamiento para diferentes tipos de variables:\n",
    "\n",
    "- Las variables categóricas necesitan ser codificadas utilizando técnicas como OneHotEncoder o OrdinalEncoder.\n",
    "- Las variables numéricas pueden ser escaladas utilizando StandardScaler o MinMaxScaler.\n",
    "- También es importante manejar los valores faltantes utilizando imputadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las variables categóricas y numéricas\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Creamos pipelines de preprocesamiento\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Usamos ColumnTransformer para combinar ambas transformaciones\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e858c",
   "metadata": {},
   "source": [
    "## Creación del Pipeline\n",
    "Ahora combinamos el preprocesamiento en un pipeline. Esto nos permite encadenar las transformaciones de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el pipeline completo\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2f679",
   "metadata": {},
   "source": [
    "## Entrenamiento de un Modelo con el Pipeline\n",
    "Ahora que tenemos el pipeline de preprocesamiento configurado, vamos a entrenar un modelo. En este caso, usaremos un **RandomForestClassifier**, un algoritmo robusto y eficiente para tareas de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb75a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creamos un nuevo pipeline que incluye el preprocesamiento y el modelo\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "\n",
    "# Entrenamos el pipeline completo en los datos de entrenamiento\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Hacemos predicciones en el conjunto de prueba\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluamos el rendimiento del modelo\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533f9c1",
   "metadata": {},
   "source": [
    "## Guardar el Pipeline Completo (Preprocesador + Modelo)\n",
    "Finalmente, guardamos el pipeline completo (incluyendo el modelo entrenado) en un archivo `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el pipeline completo (preprocesamiento + modelo entrenado)\n",
    "dump(model_pipeline, 'model_pipeline_rf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee95c5",
   "metadata": {},
   "source": [
    "## ¿Por qué es esto importante en MLOps?\n",
    "\n",
    "1. **Reproducibilidad**: Guardar el pipeline completo asegura que puedas reproducir exactamente las mismas transformaciones y predicciones en diferentes entornos.\n",
    "2. **Automatización**: En un entorno MLOps, los pipelines automatizados son clave para la integración continua y el despliegue continuo (CI/CD). Guardar el modelo entrenado con `joblib` facilita esta automatización.\n",
    "3. **Escalabilidad**: Al tener tanto el preprocesamiento como el modelo entrenado almacenados, puedes aplicarlos a nuevos datos sin necesidad de recalcular cada paso, lo que es esencial cuando se trabaja con grandes volúmenes de datos en producción.\n",
    "4. **Consistencia**: Evitamos errores derivados de la aplicación inconsistente de transformaciones de datos, garantizando que los datos en producción se traten de la misma forma que en el entrenamiento."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
